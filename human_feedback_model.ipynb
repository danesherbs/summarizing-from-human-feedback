{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import einops\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from torchtyping import TensorType\n",
    "import collections\n",
    "import json\n",
    "import linecache\n",
    "\n",
    "load_dotenv()  # take environment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_non_masked_indices(mask):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/openai/summarize-from-feedback/blob/master/summarize_from_feedback/reward_model.py\n",
    "    \"\"\"\n",
    "    bools = mask == 0\n",
    "    row_len = bools.size(-1)\n",
    "    zero_or_index = row_len * (~bools).type(torch.long) + torch.arange(\n",
    "        row_len, dtype=torch.long, device=bools.device\n",
    "    )\n",
    "    indices = torch.min(zero_or_index, dim=-1).values - 1\n",
    "    return torch.max(indices, torch.zeros([1], dtype=indices.dtype, device=mask.device))\n",
    "\n",
    "def last_non_masked_tokens(tokens, mask):\n",
    "    last_indices = last_non_masked_indices(mask)\n",
    "    last_tokens = torch.gather(tokens, dim=-1, index=last_indices[:, None])\n",
    "    return last_tokens.squeeze(-1)\n",
    "\n",
    "\n",
    "class GPTWithRewardHead(torch.nn.Module):\n",
    "    def __init__(self, mask_token_id=-100):\n",
    "        super().__init__()\n",
    "        self.gpt = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=mask_token_id).to(device)\n",
    "        self.generate = self.gpt.generate  # borrow existing generate function\n",
    "        hidden_size = self.gpt.transformer.wte.weight.shape[-1]\n",
    "        self.reward_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4 * hidden_size, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        response = self.gpt(input_ids, output_hidden_states=True, **kwargs)  # [batch_size, num_layers, hidden_dim]\n",
    "        last_hidden_state = response.hidden_states[-1]  # [batch_size, seq_len, hidden_size]\n",
    "        rewards = self.reward_network(last_hidden_state).squeeze(-1)  # [batch_size, seq_len]\n",
    "        last_rewards = last_non_masked_tokens(rewards, kwargs[\"attention_mask\"])\n",
    "        return last_rewards\n",
    "\n",
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = -100\n",
    "\n",
    "reward_model = GPTWithRewardHead().to(device)\n",
    "reward_model.load_state_dict(torch.load(\"models/reward.pt\"))\n",
    "reward_model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTWithValueHead(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, path_to_baseline_model=\"models/baseline.pt\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=self.tokenizer.eos_token_id)\n",
    "        self.gpt.load_state_dict(torch.load(path_to_baseline_model))\n",
    "        self.generate = self.gpt.generate  # borrow existing generate function\n",
    "        hidden_size = self.gpt.transformer.wte.weight.shape[-1]\n",
    "        self.value_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4 * hidden_size, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # response = self.gpt(input_ids, output_hidden_states=True)  # [batch_size, num_layers, hidden_dim]\n",
    "        response = self.gpt(input_ids)  # [batch_size, num_layers, hidden_dim]\n",
    "        # last_hidden_state = response.hidden_states[-1]  # [batch_size, seq_len, hidden_size]\n",
    "        # values = self.value_network(last_hidden_state).squeeze(-1)\n",
    "        logits = response.logits  # [batch_size, seq_len, vocab_size]\n",
    "        # return logits, values\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ref_model = GPTWithValueHead().to(device)\n",
    "ref_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "ref_model.load_state_dict(torch.load(\"models/baseline.pt\"))\n",
    "ref_model.eval()\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(model, input_ids, batch_size=50, gen_len=10, temperature=0.6):\n",
    "    with torch.no_grad():\n",
    "        samples = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[-1] + gen_len,\n",
    "            min_length=input_ids.shape[-1] + gen_len,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=len(tokenizer),\n",
    "            top_p=1.0,\n",
    "            num_return_sequences=batch_size\n",
    "        )\n",
    "        gen_samples = samples[:, input_ids.shape[-1]:]\n",
    "        sample_ids = copy.deepcopy(samples)\n",
    "        samples = tokenizer.batch_decode(samples)\n",
    "        gen_samples = tokenizer.batch_decode(gen_samples)\n",
    "    return sample_ids, samples, gen_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('Testing', return_tensors='pt').to(device)\n",
    "sample_ids, samples, gen_samples =  get_samples(model=ref_model, input_ids=input_ids, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test.\n"
     ]
    }
   ],
   "source": [
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        return gen_sample.count('.')\n",
    "\n",
    "def reward_fn_test():\n",
    "    A = 'This is a test.'\n",
    "    assert reward_fn(A) == 1\n",
    "    B = '......'\n",
    "    assert reward_fn(B) ==6\n",
    "    C = 'Whatever'\n",
    "    assert reward_fn(C) == 0\n",
    "    assert reward_fn([A, B, C]) == [1, 6, 0]\n",
    "\n",
    "    print('Passed test.')\n",
    "    return\n",
    "\n",
    "reward_fn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprobs(input_ids, logits):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs = torch.gather(logprobs, -1, input_ids[:,:,None])[:,:,0]\n",
    "    return logprobs\n",
    "\n",
    "def logprobs_test(logprobs_fn):\n",
    "    input_ids = torch.randint(0, 100, (10, 10))\n",
    "    logits = torch.randn(10, 10, 100)\n",
    "    ref_logprobs = get_logprobs(input_ids, logits)\n",
    "    logprobs = logprobs_fn(input_ids, logits)\n",
    "    assert torch.allclose(logprobs, ref_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noas_log_probs_from_logits(sample_ids, logits, prefix_len):\n",
    "    return get_logprobs(sample_ids[:,prefix_len:], logits[:,prefix_len-1:-1])\n",
    "\n",
    "def log_probs_from_logits(logits: TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"], input_ids: TensorType[\"batch_size\", \"seq_len\"], prefix_len=1) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "    assert prefix_len > 0\n",
    "\n",
    "    logits = logits[:, prefix_len-1:-1]  # [batch_size, seq_len-1, vocab_size] -- ignore x_{n+1}\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1) \n",
    "    \n",
    "    input_ids = input_ids[:, prefix_len:]  # [batch_size, seq_len-1, vocab_size]  -- ignore x_0\n",
    "    input_ids = input_ids.unsqueeze(-1)  # [batch_size, seq_len-1, vocab_size, 1]\n",
    "    seq_log_probs = torch.gather(input=log_probs, dim=-1, index=input_ids)\n",
    "    seq_log_probs = seq_log_probs.squeeze(-1) # [batch_size, seq_len-1]\n",
    "    \n",
    "    return seq_log_probs\n",
    "\n",
    "def log_probs_from_logits_test():\n",
    "    input_ids = torch.randint(0, 100, (10, 10))\n",
    "    logits = torch.randn(10, 10, 100)\n",
    "    for prefix_len in range(1, 10):\n",
    "        actual = log_probs_from_logits(logits=logits, input_ids=input_ids, prefix_len=prefix_len)\n",
    "        expected = noas_log_probs_from_logits(logits=logits, sample_ids=input_ids, prefix_len=prefix_len)\n",
    "        assert torch.allclose(actual, expected)\n",
    "\n",
    "\n",
    "log_probs_from_logits_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p_logits, q_logits):\n",
    "    p_log_probs = torch.nn.functional.log_softmax(p_logits, dim=-1)\n",
    "    q_log_probs = torch.nn.functional.log_softmax(q_logits, dim=-1)\n",
    "    kl_div = torch.nn.functional.kl_div(input=q_log_probs, target=p_log_probs, reduction=\"none\", log_target=True)\n",
    "    return torch.sum(kl_div, dim=-1)\n",
    "\n",
    "def kl_divergence_test():\n",
    "    p_logits = torch.tensor([[1, 2, 3]], dtype=torch.float32)\n",
    "    q_logits = torch.tensor([[4, 5, 6]], dtype=torch.float32)\n",
    "    p = torch.nn.functional.softmax(p_logits, dim=-1)\n",
    "    q = torch.nn.functional.softmax(q_logits, dim=-1)\n",
    "    actual = kl_divergence(p_logits, q_logits)\n",
    "    expected = torch.tensor([[p[0][i] * torch.log(p[0][i] / q[0][i]) for i in range(3)]])\n",
    "    assert torch.allclose(actual, expected), f\"{actual}, {expected}\"\n",
    "\n",
    "\n",
    "kl_divergence_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(warmup_steps, total_steps, final_scale):\n",
    "    def lr_scheduler(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            return 1-(1-final_scale)*(step-warmup_steps)/(total_steps-warmup_steps)\n",
    "    \n",
    "    return lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(t, eps=1e-5):\n",
    "    t = t - t.mean()\n",
    "    t = t / (t.std() + eps)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(sample_ids, ref_logits, rewards):\n",
    "    sample_ids = einops.rearrange(sample_ids,\"(m b) t -> m b t\", b=minibatch_size)\n",
    "    ref_logits = einops.rearrange(ref_logits, \"(m b) t d -> m b t d\", b=minibatch_size)\n",
    "    rewards = einops.rearrange(rewards, \"(m b) -> m b\", b=minibatch_size)\n",
    "    for i in range(batch_size // minibatch_size):\n",
    "        yield {\"sample_ids\": sample_ids[i], \"ref_logits\": ref_logits[i], \"rewards\": rewards[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparisonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_dir=\"./comparisons\"):\n",
    "        self.path_to_dataset_dir = path_to_dataset_dir\n",
    "        self.file_names = [f\"{path_to_dataset_dir}/batch{i}.json\" for i in range(11, 21)]\n",
    "        self.file_lengths = None\n",
    "        self.seen = set()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.file_lengths is None:\n",
    "            self.file_lengths = collections.OrderedDict()\n",
    "            for file_name in self.file_names:\n",
    "                with open(file_name) as f:\n",
    "                    self.file_lengths[file_name] = sum(1 for line in f)\n",
    "\n",
    "        return sum(self.file_lengths.values())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        i = i % len(self)\n",
    "\n",
    "        if not (0 <= i < len(self)):\n",
    "            raise IndexError(\n",
    "                f\"Tried to retrieve sample at index {i}, but only indicies between 0 and {len(self)-1} modulo {len(self)} are valid.\"\n",
    "            )\n",
    "\n",
    "        cum_length = 0\n",
    "\n",
    "        for file_name in self.file_names:\n",
    "            cum_length += self.file_lengths[file_name]\n",
    "            if i < cum_length:\n",
    "                file_idx = i - cum_length + self.file_lengths[file_name]\n",
    "                line = linecache.getline(file_name, lineno=file_idx + 1)\n",
    "                payload = json.loads(line)\n",
    "                post = payload[\"info\"][\"post\"]\n",
    "                return f\"{post} TLDR:\"\n",
    "\n",
    "\n",
    "dataset = ComparisonDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_minibatches_per_epoch = 4\n",
    "minibatch_size = 5\n",
    "kl_coef = 80  # 0.2\n",
    "vf_coef = 0.3\n",
    "n_steps = 1_000  # 300\n",
    "warmup_steps = 30\n",
    "lr = 3e-5\n",
    "gen_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/danesherbs/learning-to-summarise-using-human-feedback/1359bbe84af44bdab036e0d0a8e40e99\n",
      "\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/danesherbs/learning-to-summarise-using-human-feedback/1359bbe84af44bdab036e0d0a8e40e99\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     kl loss [4000]     : (0.0, 12.157878875732422)\n",
      "COMET INFO:     loss [400]         : (-2255.92724609375, 1920.4840087890625)\n",
      "COMET INFO:     lr [4000]          : (0.0, 3e-05)\n",
      "COMET INFO:     mean_reward [1000] : (-0.25146484375, 0.87548828125)\n",
      "COMET INFO:     policy_loss [4000] : (-3939.677001953125, 2653.654541015625)\n",
      "COMET INFO:     total_loss [4000]  : (-3551.366943359375, 3077.697021484375)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (84.35 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO:     text-sample              : 1000\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=os.getenv(\"COMET_API_KEY\"),\n",
    "    project_name=\"learning-to-summarise-using-human-feedback\",\n",
    "    workspace=\"danesherbs\",\n",
    "    log_env_cpu=False,\n",
    "    log_env_gpu=False,\n",
    ")\n",
    "\n",
    "experiment.add_tag(\"human_feedback_model\")\n",
    "\n",
    "data_loader_config = {\n",
    "    \"batch_size\": 1,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "data_loader = iter(torch.utils.data.DataLoader(dataset, **data_loader_config))\n",
    "# model = GPTWithValueHead().to(device)\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "model.load_state_dict(torch.load(\"models/baseline.pt\"))\n",
    "\n",
    "batch_size = minibatch_size * n_minibatches_per_epoch\n",
    "prefix_len = input_ids.shape[-1]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = get_lr_scheduler(5, n_steps, 0.1)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "\n",
    "def get_loss(sample_ids, ref_logits, rewards, prefix_len):\n",
    "    # logits, values = model(sample_ids)\n",
    "    logits = model(sample_ids).logits\n",
    "    log_probs = log_probs_from_logits(logits=logits, input_ids=sample_ids, prefix_len=prefix_len)\n",
    "    \n",
    "    # kl loss\n",
    "    kl = kl_divergence(q_logits=logits, p_logits=ref_logits)[:,prefix_len-1:-1]\n",
    "    kl_loss = kl.mean()\n",
    "    \n",
    "    # value loss\n",
    "    # seq_len = logits.shape[1]\n",
    "    # rewards_to_go = einops.repeat(rewards, \"batch_size -> batch_size seq_len\", seq_len=seq_len)\n",
    "    # value_loss = torch.nn.functional.mse_loss(values, rewards_to_go)\n",
    "    \n",
    "    # policy loss\n",
    "    policy_loss = -(torch.sum(log_probs, dim=-1) * rewards).mean()\n",
    "    \n",
    "    # total loss\n",
    "    # loss = policy_loss + vf_coef * value_loss + kl_coef * kl_loss\n",
    "    loss = policy_loss + kl_coef * kl_loss\n",
    "    \n",
    "    # experiment.log_metric('value_loss', value_loss.item())\n",
    "    experiment.log_metric('kl loss', kl_loss.item())\n",
    "    experiment.log_metric('total_loss', loss.item())\n",
    "    experiment.log_metric('lr', lr_scheduler.get_last_lr()[0])\n",
    "    experiment.log_metric('policy_loss', policy_loss.item())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "for batch_idx in range(n_steps):\n",
    "    [prefix] = next(data_loader)\n",
    "    input_ids = tokenizer(prefix, return_tensors='pt').input_ids.to(device)\n",
    "    sample_ids, samples, gen_samples = get_samples(model, input_ids=input_ids, batch_size=batch_size, gen_len=gen_len)\n",
    "    sample_ids = sample_ids.to(device)\n",
    "    # ref_logits, _ = ref_model(sample_ids)\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            ref_logits = ref_model(sample_ids).logits.detach()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            attention_mask = torch.ones_like(sample_ids).to(device)  # all samples have the same length and therefore no masking takes place\n",
    "            rewards = reward_model(sample_ids, attention_mask=attention_mask).detach()\n",
    "            rewards_normed = whiten(rewards)\n",
    "            rewards_normed = rewards_normed\n",
    "    \n",
    "    experiment.log_metric('mean_reward', rewards.mean())\n",
    "    experiment.log_text(prefix + gen_samples[0])\n",
    "    \n",
    "    for minibatch in get_minibatches(sample_ids, ref_logits, rewards_normed):\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            loss = get_loss(**minibatch, prefix_len=prefix_len)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0, norm_type=2.0, error_if_nonfinite=True)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    if batch_idx > 0 and batch_idx % 1_000 == 0:\n",
    "        torch.save(model.state_dict(), \"models/human_feedback.pt\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = transformers.AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment').to(device)\n",
    "cls_tokenizer = transformers.AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        logits = cls_model(cls_tokenizer(gen_sample, return_tensors='pt')['input_ids'].to(device)).logits[0]\n",
    "        logprobs = F.log_softmax(logits, dim=0)\n",
    "        assert logprobs.shape == (3,)\n",
    "        return float(logprobs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nSo, I was taking a man\\'s order at the cafe I work at. He was pretty overweight, crazy hair, sunglasses with one of the arms broken off, and just seemed generally socially awkward. He orders a cupcake, which is pretty expensive, and I tell him the price is $4.26. He reacts: \"Oh wow, $4.26?\" And I reply that it will change his life.\\n\\nThe response that floored me: \"Oh, well, I\\'m going to hold you to that, if it doesn\\'t I\\'ll put it on my blog. Yeah, I\\'ve already been taking pictures of you with my phone, oh my god what am I saying.\"\\n\\nI laugh quietly and just finish the transaction in silence, pretending to not have heard him. But seriously you guys, this shit was hella awkward. And I felt super bad for this dude because it obviously just slipped out, as a joke that just turned out to be really creepy. SO WHAT THE FUCK AM I SUPPOSED TO DO?\\n\\nTLDR: I was in a car with a guy and he was in a car with a guy and he was in a car with a guy and he was in']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "So, I was taking a man's order at the cafe I work at. He was pretty overweight, crazy hair, sunglasses with one of the arms broken off, and just seemed generally socially awkward. He orders a cupcake, which is pretty expensive, and I tell him the price is $4.26. He reacts: \"Oh wow, $4.26?\" And I reply that it will change his life.\n",
    "\n",
    "The response that floored me: \"Oh, well, I'm going to hold you to that, if it doesn't I'll put it on my blog. Yeah, I've already been taking pictures of you with my phone, oh my god what am I saying.\"\n",
    "\n",
    "I laugh quietly and just finish the transaction in silence, pretending to not have heard him. But seriously you guys, this shit was hella awkward. And I felt super bad for this dude because it obviously just slipped out, as a joke that just turned out to be really creepy. SO WHAT THE FUCK AM I SUPPOSED TO DO?\n",
    "\n",
    "TLDR:\"\"\"\n",
    "\n",
    "\n",
    "test_input_ids = torch.tensor(tokenizer([prompt]).input_ids).to(device)\n",
    "\n",
    "_, generated_text, _ = get_samples(\n",
    "    model,\n",
    "    test_input_ids,\n",
    "    batch_size=1,\n",
    "    gen_len=30,\n",
    "    temperature=1e-3,\n",
    ")\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
