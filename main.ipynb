{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import einops\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from torchtyping import TensorType\n",
    "\n",
    "load_dotenv()  # take environment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cuda:3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTWithValueHead(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=self.tokenizer.eos_token_id)\n",
    "        self.generate = self.gpt.generate  # borrow existing generate function\n",
    "        hidden_size = self.gpt.transformer.wte.weight.shape[-1]\n",
    "        self.value_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4 * hidden_size, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        response = self.gpt(input_ids, output_hidden_states=True)  # [batch_size, num_layers, hidden_dim]\n",
    "        last_hidden_state = response.hidden_states[-1]  # [batch_size, seq_len, hidden_size]\n",
    "        values = self.value_network(last_hidden_state).squeeze(-1)\n",
    "        logits = response.logits  # [batch_size, seq_len, vocab_size]\n",
    "        return logits, values\n",
    "\n",
    "\n",
    "ref_model = GPTWithValueHead().to(device)\n",
    "ref_model.eval()\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(model, input_ids, batch_size=50, gen_len=10):\n",
    "    with torch.no_grad():\n",
    "        samples = model.generate(input_ids, max_length=input_ids.shape[-1]+gen_len, min_length=input_ids.shape[-1]+gen_len, do_sample=True, temperature=0.6, top_k=len(tokenizer), top_p=1.0, num_return_sequences=batch_size)\n",
    "        gen_samples = samples[:, input_ids.shape[-1]:]\n",
    "        sample_ids = copy.deepcopy(samples)\n",
    "        samples = tokenizer.batch_decode(samples)\n",
    "        gen_samples = tokenizer.batch_decode(gen_samples)\n",
    "    return sample_ids, samples, gen_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('Testing', return_tensors='pt').to(device)\n",
    "sample_ids, samples, gen_samples =  get_samples(model=ref_model, input_ids=input_ids, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test.\n"
     ]
    }
   ],
   "source": [
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        return gen_sample.count('.')\n",
    "\n",
    "def reward_fn_test():\n",
    "    A = 'This is a test.'\n",
    "    assert reward_fn(A) == 1\n",
    "    B = '......'\n",
    "    assert reward_fn(B) ==6\n",
    "    C = 'Whatever'\n",
    "    assert reward_fn(C) == 0\n",
    "    assert reward_fn([A, B, C]) == [1, 6, 0]\n",
    "\n",
    "    print('Passed test.')\n",
    "    return\n",
    "\n",
    "reward_fn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprobs(input_ids, logits):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs = torch.gather(logprobs, -1, input_ids[:,:,None])[:,:,0]\n",
    "    return logprobs\n",
    "\n",
    "def logprobs_test(logprobs_fn):\n",
    "    input_ids = torch.randint(0, 100, (10, 10))\n",
    "    logits = torch.randn(10, 10, 100)\n",
    "    ref_logprobs = get_logprobs(input_ids, logits)\n",
    "    logprobs = logprobs_fn(input_ids, logits)\n",
    "    assert torch.allclose(logprobs, ref_logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noas_log_probs_from_logits(sample_ids, logits, prefix_len):\n",
    "    return get_logprobs(sample_ids[:,prefix_len:], logits[:,prefix_len-1:-1])\n",
    "\n",
    "def log_probs_from_logits(logits: TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"], input_ids: TensorType[\"batch_size\", \"seq_len\"], prefix_len=1) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "    assert prefix_len > 0\n",
    "\n",
    "    logits = logits[:, prefix_len-1:-1]  # [batch_size, seq_len-1, vocab_size] -- ignore x_{n+1}\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1) \n",
    "    \n",
    "    input_ids = input_ids[:, prefix_len:]  # [batch_size, seq_len-1, vocab_size]  -- ignore x_0\n",
    "    input_ids = input_ids.unsqueeze(-1)  # [batch_size, seq_len-1, vocab_size, 1]\n",
    "    seq_log_probs = torch.gather(input=log_probs, dim=-1, index=input_ids)\n",
    "    seq_log_probs = seq_log_probs.squeeze(-1) # [batch_size, seq_len-1]\n",
    "    \n",
    "    return seq_log_probs\n",
    "\n",
    "def log_probs_from_logits_test():\n",
    "    input_ids = torch.randint(0, 100, (10, 10))\n",
    "    logits = torch.randn(10, 10, 100)\n",
    "    for prefix_len in range(1, 10):\n",
    "        actual = log_probs_from_logits(logits=logits, input_ids=input_ids, prefix_len=prefix_len)\n",
    "        expected = noas_log_probs_from_logits(logits=logits, sample_ids=input_ids, prefix_len=prefix_len)\n",
    "        assert torch.allclose(actual, expected)\n",
    "\n",
    "\n",
    "log_probs_from_logits_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p_logits, q_logits):\n",
    "    p_log_probs = torch.nn.functional.log_softmax(p_logits, dim=-1)\n",
    "    q_log_probs = torch.nn.functional.log_softmax(q_logits, dim=-1)\n",
    "    kl_div = torch.nn.functional.kl_div(input=q_log_probs, target=p_log_probs, reduction=\"none\", log_target=True)\n",
    "    return torch.sum(kl_div, dim=-1)\n",
    "\n",
    "def kl_divergence_test():\n",
    "    p_logits = torch.tensor([[1, 2, 3]], dtype=torch.float32)\n",
    "    q_logits = torch.tensor([[4, 5, 6]], dtype=torch.float32)\n",
    "    p = torch.nn.functional.softmax(p_logits, dim=-1)\n",
    "    q = torch.nn.functional.softmax(q_logits, dim=-1)\n",
    "    actual = kl_divergence(p_logits, q_logits)\n",
    "    expected = torch.tensor([[p[0][i] * torch.log(p[0][i] / q[0][i]) for i in range(3)]])\n",
    "    assert torch.allclose(actual, expected), f\"{actual}, {expected}\"\n",
    "\n",
    "\n",
    "kl_divergence_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(warmup_steps, total_steps, final_scale):\n",
    "    def lr_scheduler(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            return 1-(1-final_scale)*(step-warmup_steps)/(total_steps-warmup_steps)\n",
    "    return lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(t, eps=1e-5):\n",
    "    t = t - t.mean()\n",
    "    t = t/(t.std()+eps)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tensor_left(t: TensorType[\"batch_size\", \"seq_len\"]) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "    \"\"\"\n",
    "    Shifts tensor left one and fills with zeros: [v_0, v_1, ..., v_n] -> [v_1, ..., v_n, 0].\n",
    "    \n",
    "    Note: you probably want to detach the result of this function.\n",
    "    \"\"\"\n",
    "    shifted = torch.zeros_like(t)\n",
    "    shifted[:, :-1] = t[:, 1:]\n",
    "    return shifted\n",
    "\n",
    "def shift_tensor_left_test():\n",
    "    t = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "    shifted_t = shift_tensor_left(t)\n",
    "    assert torch.allclose(shifted_t, torch.tensor([[2, 3, 4, 0], [6, 7, 8, 0]]))\n",
    "\n",
    "\n",
    "shift_tensor_left_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_to_go_from_rewards_per_timestep(rewards_per_timestep: TensorType[\"batch_size\", \"seq_len\"]) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "    flipped = torch.fliplr(rewards_per_timestep)\n",
    "    flipped_cumsum = torch.cumsum(flipped, dim=-1)\n",
    "    rewards_to_go = torch.fliplr(flipped_cumsum)\n",
    "    return rewards_to_go\n",
    "\n",
    "def rewards_to_go_from_rewards_per_timestep_test():\n",
    "    rewards_per_timestep = torch.tensor([\n",
    "        [0.5, 1.0, 0.5],\n",
    "        [1.5, 1.0, 1.0],\n",
    "    ])\n",
    "    actual = rewards_to_go_from_rewards_per_timestep(rewards_per_timestep)\n",
    "    expected = torch.tensor([\n",
    "        [2.0, 1.5, 0.5],\n",
    "        [3.5, 2.0, 1.0],\n",
    "    ])\n",
    "    assert torch.allclose(actual, expected)\n",
    "\n",
    "\n",
    "rewards_to_go_from_rewards_per_timestep_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_per_timestep_from_rewards(rewards: TensorType[\"batch_size\"], seq_len: int) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "    batch_size = rewards.shape[0]\n",
    "    rewards_per_timestep = torch.zeros(batch_size, seq_len).to(device)\n",
    "    rewards_per_timestep[:, -1] = rewards\n",
    "    return rewards_per_timestep\n",
    "\n",
    "def rewards_per_timestep_from_rewards_test():\n",
    "    rewards = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "    seq_len = 3\n",
    "    actual = rewards_per_timestep_from_rewards(rewards, seq_len)\n",
    "    expected = torch.tensor([\n",
    "        [0.0, 0.0, 1.0],\n",
    "        [0.0, 0.0, 2.0],\n",
    "        [0.0, 0.0, 3.0],\n",
    "    ]).to(device)\n",
    "    assert torch.allclose(actual, expected)\n",
    "\n",
    "\n",
    "rewards_per_timestep_from_rewards_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(sample_ids, old_log_probs, ref_logits, old_values, rewards):\n",
    "    sample_ids = einops.rearrange(sample_ids,'(m b) t -> m b t', b=minibatch_size)\n",
    "    old_log_probs = einops.rearrange(old_log_probs, '(m b) t -> m b t', b=minibatch_size)\n",
    "    ref_logits = einops.rearrange(ref_logits, '(m b) t d -> m b t d', b=minibatch_size)\n",
    "    old_values = einops.rearrange(old_values, '(m b) t -> m b t', b=minibatch_size)\n",
    "    rewards = einops.rearrange(rewards, '(m b) -> m b', b=minibatch_size)\n",
    "    for i in range(batch_size//minibatch_size):\n",
    "        yield {'sample_ids': sample_ids[i], 'old_log_probs': old_log_probs[i], 'ref_logits': ref_logits[i], 'old_values': old_values[i], 'rewards': rewards[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_minibatches_per_epoch = 4\n",
    "minibatch_size=20\n",
    "n_epochs = 40\n",
    "ent_coef = 0.0  # .001\n",
    "kl_coef = 80  # 0.2\n",
    "vf_coef = 0.3\n",
    "n_steps = 300\n",
    "warmup_steps = 10\n",
    "lr = 3e-5\n",
    "gen_len=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/danesherbs/learning-to-summarise-using-human-feedback/c15d8ee3c5db482c9bc4e2c55ff62853\n",
      "\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/danesherbs/learning-to-summarise-using-human-feedback/c15d8ee3c5db482c9bc4e2c55ff62853\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     kl loss [1200]        : (2.3543589300345502e-09, 0.08422788232564926)\n",
      "COMET INFO:     loss [120]            : (-46.14427185058594, 15.33053970336914)\n",
      "COMET INFO:     lr [1200]             : (0.0, 3e-05)\n",
      "COMET INFO:     mean_reward [300]     : (1.2625000476837158, 4.200000286102295)\n",
      "COMET INFO:     pg_clipfrac [1200]    : (0.0, 0.043333333333333335)\n",
      "COMET INFO:     pg_loss [1200]        : (-58.8792724609375, 24.934045791625977)\n",
      "COMET INFO:     total_loss [1200]     : (-55.471458435058594, 25.04931640625)\n",
      "COMET INFO:     value_net_loss [1200] : (0.020342804491519928, 0.6083584427833557)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (24.31 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO:     text-sample              : 300\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=os.getenv(\"COMET_API_KEY\"),\n",
    "    project_name=\"learning-to-summarise-using-human-feedback\",\n",
    "    workspace=\"danesherbs\",\n",
    "    log_env_cpu=False,\n",
    "    log_env_gpu=False,\n",
    ")\n",
    "\n",
    "model = GPTWithValueHead().to(device)\n",
    "prefix = \"This is\"\n",
    "input_ids = tokenizer(prefix, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "batch_size = minibatch_size*n_minibatches_per_epoch\n",
    "prefix_len=input_ids.shape[-1]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = get_lr_scheduler(5, n_steps, 0.1)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "\n",
    "def get_loss(sample_ids, old_log_probs, ref_logits, old_values, rewards, prefix_len, clip_range=.2):\n",
    "    logits, est_values = model(sample_ids)\n",
    "    log_probs = log_probs_from_logits(logits=logits, input_ids=sample_ids, prefix_len=prefix_len)\n",
    "    \n",
    "    kl = kl_divergence(q_logits=logits, p_logits=ref_logits)[:,prefix_len-1:-1]\n",
    "    kl_loss = kl.mean()\n",
    "    \n",
    "    def get_advantages(values, prefix_len):\n",
    "        one_step_q_est = torch.cat((est_values[:,prefix_len:-1].detach(), rewards[:,None]), dim=-1)\n",
    "        # s_0a_0r_0s_1a_1r_1s_2a_2r_2s_3a_3r_3s_4\n",
    "        #          v_1 ---- v_2 ---- v_3 ---- 0\n",
    "        #        + 0   ---- 0   ---- 0   ---- r_3\n",
    "        \n",
    "        zero_step_value_est = est_values[:,prefix_len-1:-1]\n",
    "        # s_0a_0r_0s_1a_1r_1s_2a_2r_2s_3a_3r_3s_4\n",
    "        # v_0 ---- v_1 ---- v_2 ---- v_3\n",
    "    \n",
    "        advantages = one_step_q_est - zero_step_value_est\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "    def my_get_advantages(values, prefix_len):\n",
    "        seq_len = logits.shape[1]\n",
    "        rewards_per_timestep = rewards_per_timestep_from_rewards(rewards, seq_len)\n",
    "        rewards_to_go = rewards_to_go_from_rewards_per_timestep(rewards_per_timestep)\n",
    "        return None\n",
    "    \n",
    "    def get_value_net_loss(est_values):\n",
    "        shifted_values = shift_tensor_left(est_values).detach()\n",
    "        seq_len = logits.shape[1]\n",
    "        rewards_per_timestep = rewards_per_timestep_from_rewards(rewards, seq_len)\n",
    "        value_net_loss = ((est_values - (rewards_per_timestep + shifted_values)) ** 2).mean()\n",
    "        return value_net_loss\n",
    "    \n",
    "    advantages = get_advantages(est_values, prefix_len)\n",
    "    value_net_loss = (advantages**2).mean()\n",
    "\n",
    "    # value_net_loss = get_value_net_loss(est_values)\n",
    "\n",
    "    # Noa's policy gradient loss\n",
    "    ratio = torch.exp(log_probs - old_log_probs)\n",
    "    pg_losses1 = -advantages.detach() * ratio\n",
    "    pg_losses2 = -advantages.detach() * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    pg_loss = torch.max(pg_losses1, pg_losses2).mean()\n",
    "    pg_clipfrac = torch.mean(torch.gt(pg_losses2, pg_losses1).double())\n",
    "    # end\n",
    "\n",
    "    # My policy gradient loss\n",
    "    # seq_len = logits.shape[1]\n",
    "    # rewards_per_timestep = rewards_per_timestep_from_rewards(rewards, seq_len)\n",
    "    # rewards_to_go = rewards_to_go_from_rewards_per_timestep(rewards_per_timestep)\n",
    "    # rewards_to_go = rewards_to_go[:, prefix_len:]  # ignore rewards of prefix\n",
    "    # assert log_probs.shape == rewards_to_go.shape\n",
    "    # baseline = est_values[:, prefix_len:]\n",
    "    # assert log_probs.shape == baseline.shape\n",
    "    # pg_loss = -torch.sum(log_probs * (rewards_to_go - baseline), dim=-1).mean()\n",
    "    \n",
    "    pg_loss = -(torch.sum(log_probs, dim=-1) * rewards).mean()\n",
    "    # end\n",
    "\n",
    "    loss = pg_loss + vf_coef * value_net_loss + kl_coef * kl_loss\n",
    "    \n",
    "    experiment.log_metric('pg_clipfrac', pg_clipfrac.item())\n",
    "    experiment.log_metric('value_net_loss', value_net_loss.item())\n",
    "    experiment.log_metric('kl loss', kl_loss.item())\n",
    "    experiment.log_metric('total_loss', loss.item())\n",
    "    experiment.log_metric('lr', lr_scheduler.get_last_lr()[0])\n",
    "    experiment.log_metric('pg_loss', pg_loss.item())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "for batch_idx in range(n_steps):\n",
    "    sample_ids, samples, gen_samples = get_samples(model, input_ids=input_ids, batch_size=batch_size, gen_len=gen_len)\n",
    "    sample_ids = sample_ids.to(device)\n",
    "    experiment.log_text(gen_samples[0])\n",
    "    old_logits, old_values = model(sample_ids)\n",
    "    old_logits, old_values = old_logits.detach(), old_values.detach()\n",
    "    old_log_probs = get_logprobs(sample_ids[:,input_ids.shape[-1]:], old_logits[:,input_ids.shape[-1]-1:-1]).detach()\n",
    "    ref_logits, _ = ref_model(sample_ids)\n",
    "    ref_logits = ref_logits.detach()\n",
    "    \n",
    "    rewards = torch.tensor(reward_fn(samples), dtype=torch.float32).to(device)\n",
    "    experiment.log_metric('mean_reward', rewards.mean())\n",
    "    rewards = whiten(rewards)  \n",
    "    \n",
    "    for epoch in range(1):\n",
    "        for minibatch in get_minibatches(sample_ids, old_log_probs, ref_logits, old_values, rewards):\n",
    "            loss = get_loss(**minibatch, prefix_len=2, clip_range=.2)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0, norm_type=2.0, error_if_nonfinite=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = transformers.AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment').to(device)\n",
    "cls_tokenizer = transformers.AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        logits = cls_model(cls_tokenizer(gen_sample, return_tensors='pt')['input_ids'].to(device)).logits[0]\n",
    "        logprobs = F.log_softmax(logits, dim=0)\n",
    "        assert logprobs.shape == (3,)\n",
    "        return float(logprobs[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
