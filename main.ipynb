{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import einops\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from torchtyping import TensorType\n",
    "\n",
    "load_dotenv()  # take environment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTWithValueHead(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.gpt = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=self.tokenizer.eos_token_id)\n",
    "        self.generate = self.gpt.generate  # borrow existing generate function\n",
    "        hidden_size = self.gpt.transformer.wte.weight.shape[-1]\n",
    "        self.value_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4 * hidden_size, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        response = self.gpt(input_ids, output_hidden_states=True)  # [batch_size, num_layers, hidden_dim]\n",
    "        last_hidden_state = response.hidden_states[-1]  # [batch_size, seq_len, hidden_size]\n",
    "        values = self.value_network(last_hidden_state).squeeze(-1)\n",
    "        logits = response.logits  # [batch_size, seq_len, vocab_size]\n",
    "        return logits, values\n",
    "\n",
    "\n",
    "ref_model = GPTWithValueHead().to(device)\n",
    "ref_model.eval()\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(model, input_ids, batch_size=50, gen_len=10):\n",
    "    with torch.no_grad():\n",
    "        samples = model.generate(input_ids, max_length=input_ids.shape[-1]+gen_len, min_length=input_ids.shape[-1]+gen_len, do_sample=True, temperature=0.6, top_k=len(tokenizer), top_p=1.0, num_return_sequences=batch_size)\n",
    "        gen_samples = samples[:, input_ids.shape[-1]:]\n",
    "        sample_ids = copy.deepcopy(samples)\n",
    "        samples = tokenizer.batch_decode(samples)\n",
    "        gen_samples = tokenizer.batch_decode(gen_samples)\n",
    "    return sample_ids, samples, gen_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('Testing', return_tensors='pt').to(device)\n",
    "sample_ids, samples, gen_samples =  get_samples(model=ref_model, input_ids=input_ids, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        return gen_sample.count('.')\n",
    "\n",
    "def reward_fn_test():\n",
    "    A = 'This is a test.'\n",
    "    assert reward_fn(A) == 1\n",
    "    B = '......'\n",
    "    assert reward_fn(B) ==6\n",
    "    C = 'Whatever'\n",
    "    assert reward_fn(C) == 0\n",
    "    assert reward_fn([A, B, C]) == [1, 6, 0]\n",
    "\n",
    "    print('Passed test.')\n",
    "    return\n",
    "\n",
    "reward_fn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprobs(input_ids, logits):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs = torch.gather(logprobs, -1, input_ids[:,:,None])[:,:,0]\n",
    "    return logprobs\n",
    "\n",
    "def logprobs_test(logprobs_fn):\n",
    "    input_ids = torch.randint(0, 100, (10, 10))\n",
    "    logits = torch.randn(10, 10, 100)\n",
    "    ref_logprobs = get_logprobs(input_ids, logits)\n",
    "    logprobs = logprobs_fn(input_ids, logits)\n",
    "    assert torch.allclose(logprobs, ref_logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noas_log_probs_from_logits(sample_ids, logits, prefix_len):\n",
    "    return get_logprobs(sample_ids[:,prefix_len:], logits[:,prefix_len-1:-1])\n",
    "\n",
    "def log_probs_from_logits(logits: TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"], input_ids: TensorType[\"batch_size\", \"seq_len\"], prefix_len=1) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "    assert prefix_len > 0\n",
    "\n",
    "    logits = logits[:, prefix_len-1:-1]  # [batch_size, seq_len-1, vocab_size] -- ignore x_{n+1}\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1) \n",
    "    \n",
    "    input_ids = input_ids[:, prefix_len:]  # [batch_size, seq_len-1, vocab_size]  -- ignore x_0\n",
    "    input_ids = input_ids.unsqueeze(-1)  # [batch_size, seq_len-1, vocab_size, 1]\n",
    "    seq_log_probs = torch.gather(input=log_probs, dim=-1, index=input_ids)\n",
    "    seq_log_probs = seq_log_probs.squeeze(-1) # [batch_size, seq_len-1]\n",
    "    \n",
    "    return seq_log_probs\n",
    "\n",
    "def log_probs_from_logits_test():\n",
    "    input_ids = torch.randint(0, 100, (10, 10))\n",
    "    logits = torch.randn(10, 10, 100)\n",
    "    for prefix_len in range(1, 10):\n",
    "        actual = log_probs_from_logits(logits=logits, input_ids=input_ids, prefix_len=prefix_len)\n",
    "        expected = noas_log_probs_from_logits(logits=logits, sample_ids=input_ids, prefix_len=prefix_len)\n",
    "        assert torch.allclose(actual, expected)\n",
    "\n",
    "\n",
    "log_probs_from_logits_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p_logits, q_logits):\n",
    "    p_log_probs = torch.nn.functional.log_softmax(p_logits, dim=-1)\n",
    "    q_log_probs = torch.nn.functional.log_softmax(q_logits, dim=-1)\n",
    "    kl_div = torch.nn.functional.kl_div(input=q_log_probs, target=p_log_probs, reduction=\"none\", log_target=True)\n",
    "    return torch.sum(kl_div, dim=-1)\n",
    "\n",
    "def kl_divergence_test():\n",
    "    p_logits = torch.tensor([[1, 2, 3]], dtype=torch.float32)\n",
    "    q_logits = torch.tensor([[4, 5, 6]], dtype=torch.float32)\n",
    "    p = torch.nn.functional.softmax(p_logits, dim=-1)\n",
    "    q = torch.nn.functional.softmax(q_logits, dim=-1)\n",
    "    actual = kl_divergence(p_logits, q_logits)\n",
    "    expected = torch.tensor([[p[0][i] * torch.log(p[0][i] / q[0][i]) for i in range(3)]])\n",
    "    assert torch.allclose(actual, expected), f\"{actual}, {expected}\"\n",
    "\n",
    "\n",
    "kl_divergence_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(warmup_steps, total_steps, final_scale):\n",
    "    def lr_scheduler(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            return 1-(1-final_scale)*(step-warmup_steps)/(total_steps-warmup_steps)\n",
    "    \n",
    "    return lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(t, eps=1e-5):\n",
    "    t = t - t.mean()\n",
    "    t = t / (t.std() + eps)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(sample_ids, ref_logits, rewards):\n",
    "    sample_ids = einops.rearrange(sample_ids,\"(m b) t -> m b t\", b=minibatch_size)\n",
    "    ref_logits = einops.rearrange(ref_logits, \"(m b) t d -> m b t d\", b=minibatch_size)\n",
    "    rewards = einops.rearrange(rewards, \"(m b) -> m b\", b=minibatch_size)\n",
    "    for i in range(batch_size // minibatch_size):\n",
    "        yield {\"sample_ids\": sample_ids[i], \"ref_logits\": ref_logits[i], \"rewards\": rewards[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_minibatches_per_epoch = 4\n",
    "minibatch_size = 20\n",
    "kl_coef = 80  # 0.2\n",
    "vf_coef = 0.3\n",
    "n_steps = 300  # 300\n",
    "warmup_steps = 10\n",
    "lr = 3e-5\n",
    "gen_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=os.getenv(\"COMET_API_KEY\"),\n",
    "    project_name=\"learning-to-summarise-using-human-feedback\",\n",
    "    workspace=\"danesherbs\",\n",
    "    log_env_cpu=False,\n",
    "    log_env_gpu=False,\n",
    ")\n",
    "\n",
    "model = GPTWithValueHead().to(device)\n",
    "prefix = \"This is\"\n",
    "input_ids = tokenizer(prefix, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "batch_size = minibatch_size * n_minibatches_per_epoch\n",
    "prefix_len = input_ids.shape[-1]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = get_lr_scheduler(5, n_steps, 0.1)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "\n",
    "def get_loss(sample_ids, ref_logits, rewards, prefix_len):\n",
    "    logits, values = model(sample_ids)\n",
    "    log_probs = log_probs_from_logits(logits=logits, input_ids=sample_ids, prefix_len=prefix_len)\n",
    "    \n",
    "    # kl loss\n",
    "    kl = kl_divergence(q_logits=logits, p_logits=ref_logits)[:,prefix_len-1:-1]\n",
    "    kl_loss = kl.mean()\n",
    "    \n",
    "    # value loss\n",
    "    seq_len = logits.shape[1]\n",
    "    rewards_to_go = einops.repeat(rewards, \"batch_size -> batch_size seq_len\", seq_len=seq_len)\n",
    "    value_loss = torch.nn.functional.mse_loss(values, rewards_to_go)\n",
    "    \n",
    "    # policy loss\n",
    "    policy_loss = -(torch.sum(log_probs, dim=-1) * rewards).mean()\n",
    "    \n",
    "    # total loss\n",
    "    loss = policy_loss + vf_coef * value_loss + kl_coef * kl_loss\n",
    "    \n",
    "    experiment.log_metric('value_loss', value_loss.item())\n",
    "    experiment.log_metric('kl loss', kl_loss.item())\n",
    "    experiment.log_metric('total_loss', loss.item())\n",
    "    experiment.log_metric('lr', lr_scheduler.get_last_lr()[0])\n",
    "    experiment.log_metric('policy_loss', policy_loss.item())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "for batch_idx in range(n_steps):\n",
    "    sample_ids, samples, gen_samples = get_samples(model, input_ids=input_ids, batch_size=batch_size, gen_len=gen_len)\n",
    "    sample_ids = sample_ids.to(device)\n",
    "    ref_logits, _ = ref_model(sample_ids)\n",
    "    ref_logits = ref_logits.detach()\n",
    "    \n",
    "    rewards = torch.tensor(reward_fn(samples), dtype=torch.float32).to(device)\n",
    "    rewards_normed = whiten(rewards)\n",
    "    \n",
    "    experiment.log_text(prefix + gen_samples[0])\n",
    "    experiment.log_metric('mean_reward', rewards.mean())\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        for minibatch in get_minibatches(sample_ids, ref_logits, rewards_normed):\n",
    "            loss = get_loss(**minibatch, prefix_len=prefix_len)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0, norm_type=2.0, error_if_nonfinite=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = transformers.AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment').to(device)\n",
    "cls_tokenizer = transformers.AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        logits = cls_model(cls_tokenizer(gen_sample, return_tensors='pt')['input_ids'].to(device)).logits[0]\n",
    "        logprobs = F.log_softmax(logits, dim=0)\n",
    "        assert logprobs.shape == (3,)\n",
    "        return float(logprobs[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
