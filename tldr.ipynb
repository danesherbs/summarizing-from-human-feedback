{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import torch\n",
    "import json\n",
    "import linecache\n",
    "import random\n",
    "import json\n",
    "import transformers\n",
    "import einops\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tldr_dataset(n_samples=100_000):\n",
    "    linecount = 0\n",
    "    with open(\"tldr/raw_tldr.jsonl\", \"r\") as raw_file:\n",
    "        with open(\"tldr/cleaned_tldr.jsonl\", \"w\") as cleaned_file:\n",
    "            for json_str in tqdm(raw_file):\n",
    "                post = json.loads(json_str)\n",
    "                if 24 < post[\"summary_len\"] < 48:\n",
    "                    cleaned_file.write(json_str)\n",
    "                    linecount += 1\n",
    "    \n",
    "    assert n_samples <= linecount\n",
    "\n",
    "    random_idxs = random.sample(range(linecount), k=n_samples)\n",
    "    with open(\"tldr/tldr.jsonl\", \"w\") as file:\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            line = linecache.getline(\"tldr/cleaned_tldr.jsonl\", lineno=random_idxs[i])\n",
    "            file.write(line)\n",
    "\n",
    "\n",
    "# create_tldr_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLDRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_tldr_dataset):\n",
    "        self.fname = path_to_tldr_dataset\n",
    "        self.len = None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.len is None:\n",
    "            with open(self.fname) as f:\n",
    "                self.len = sum(1 for line in f)\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        i = i % len(self)\n",
    "        if not (0 <= i < len(self)):\n",
    "            raise IndexError(f\"Tried to retrieve sample at index {i}, but only indicies between 0 and {len(self)-1} modulo {len(self)} are valid.\")\n",
    "        line = linecache.getline(self.fname, lineno=i+1)\n",
    "        post = json.loads(line)\n",
    "        return post[\"normalizedBody\"]\n",
    "\n",
    "\n",
    "dataset = TLDRDataset(path_to_tldr_dataset=\"tldr/tldr.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = -100\n",
    "\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Brand new to Ableton. Loving it so far as I came from a bit so user friendly daw before this. My issue is: when I record a track with no record quantization on... and then choose not to quantize after to give it that swung/human feel... how do I quantize another midi track to the first tracks 'off the grid' timing.? \\n Because the first track (the bass line) is what I formed the section I'm working on (and have been building up from), if I quantize everything else to a super high value grid, that will obviously get them close. But the track will sound slightly messier if everything else remains on the grid (and off from the 'natural' timing of the first bass track). \\n TLDR: go only if you want a higher value capacitor and square root to the 'exotic' scale of the second track (I'm sure the bass line didn't even sound good). \\n<|endoftext|>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sample(model):\n",
    "    generate_length = 10\n",
    "    prompt = \"Brand new to Ableton. Loving it so far as I came from a bit so user friendly daw before this. My issue is: when I record a track with no record quantization on... and then choose not to quantize after to give it that swung/human feel... how do I quantize another midi track to the first tracks 'off the grid' timing. ? \\n Because the first track (the bass line) is what I formed the section I'm working on (and have been building up from), if I quantize everything else to a super high value grid, that will obviously get them close. But the track will sound slightly messier if everything else remains on the grid (and off from the 'natural' timing of the first bass track). \\n TLDR\"\n",
    "    \n",
    "    input_ids = tokenizer(\n",
    "        [prompt],\n",
    "        max_length=256,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        \n",
    "    ).input_ids.to(DEVICE)\n",
    "\n",
    "    response_ids = model.generate(\n",
    "        input_ids,\n",
    "        min_length=input_ids.shape[-1] + generate_length,\n",
    "        max_length=input_ids.shape[-1] + 10 * generate_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=len(tokenizer),\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "    [decoded] = tokenizer.batch_decode(response_ids)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "generate_sample(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch,\n",
    "        max_length=256,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "data_loader_config = {\n",
    "    \"batch_size\": 16,\n",
    "    \"shuffle\": True,\n",
    "    \"collate_fn\": collate_fn,\n",
    "}\n",
    "\n",
    "num_train = int(0.95 * len(dataset))\n",
    "num_val = len(dataset) - num_train\n",
    "\n",
    "data_train, data_val = torch.utils.data.random_split(dataset, (num_train, num_val))\n",
    "train_data_loader = torch.utils.data.DataLoader(data_train, **data_loader_config)\n",
    "val_data_loader = torch.utils.data.DataLoader(data_val, **data_loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/danesherbs/learning-to-summarise-using-human-feedback/3babb834fc9e4b48ba2057d09e3b5181\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [45]        : (3.039628744125366, 3.8634068965911865)\n",
      "COMET INFO:     lr [450]         : (3.0000000000000004e-09, 3e-05)\n",
      "COMET INFO:     train loss [450] : (3.039628744125366, 3.967376947402954)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (24.31 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     text-sample              : 450\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/danesherbs/learning-to-summarise-using-human-feedback/16277fd45e294600b357f996850b1803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_data_loader, epochs=1, lr=1e-3, comet_experiment=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=20)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for inputs in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #  I    am    a    dog    [EOS]    <--- original\n",
    "            #  I    am    a    dog             <--- inputs (shifted internally)\n",
    "            #  am   a     dog  [EOS]           <--- targets (shifted internally)\n",
    "            input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "            loss = model(input_ids, attention_mask=attention_mask, labels=input_ids).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            if comet_experiment is not None:\n",
    "                comet_experiment.log_metric('train loss', float(loss))\n",
    "                experiment.log_metric('lr', optimizer.param_groups[0]['lr'])\n",
    "                experiment.log_text(generate_sample(model))\n",
    "    \n",
    "    if comet_experiment is not None:\n",
    "        comet_experiment.end()\n",
    "\n",
    "\n",
    "experiment = comet_ml.Experiment(\n",
    "    api_key=os.getenv(\"COMET_API_KEY\"),\n",
    "    project_name=\"learning-to-summarise-using-human-feedback\",\n",
    "    workspace=\"danesherbs\",\n",
    "    log_env_cpu=False,\n",
    "    log_env_gpu=False,\n",
    ")\n",
    "\n",
    "train(model, train_data_loader, comet_experiment=experiment, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After looking into internet prices it seems Slingshot has been overcharging my account on a pricing error. \\n The price should be: broadband plan + iTalk = bill. \\n Where broadband plan is adjusted to the current price and iTalk is $11 and only available to legacy customers (no new customers can join iTalk). \\n Instead the price has just been fixed at $94 for who knows how long. For some reason iTalk does not appear on the bill, they just charge $94 for the broadband plan. Current prices for the same plan of unlimited, unbundled ADSL broadband is $79 plus $11 for iTalk. So the cost should be $90 per month, not $94. \\n I am inclined to ask Slingshot to refund that $4 a month they have been charging, but I have no idea how long this has been going on or when the price for unbundled, unlimited broadband with them dropped to less than $83. Also worried that others will be experiencing this, in that case Slingshot should really make a refund to everyone affected. \\n TL;DR:  Slingshot may not be updating the broadband prices for existing customers and instead is billing them extra. This may only apply to legacy iTalk customers. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, comet_experiment=None):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    \n",
    "    for step, inputs in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            batch_loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "            loss += batch_loss\n",
    "\n",
    "        if comet_experiment is not None:\n",
    "            comet_experiment.log_metric('batch test loss', float(loss))\n",
    "    \n",
    "    if comet_experiment is not None:\n",
    "        comet_experiment.log_metric('test loss', float(loss) / (step + 1))\n",
    "        comet_experiment.end()\n",
    "\n",
    "\n",
    "evaluate(model, val_data_loader, comet_experiment=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"baseline.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
